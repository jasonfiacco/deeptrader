{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import numpy\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "warnings.warn = warn\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "numpy.seterr(divide = 'ignore') \n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.abspath('')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Episodes:   0%|          | 0/1 [00:00, reward=0.00, ts/ep=0, sec/ep=0.00, ms/ts=0.0, agent=0.0%]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "import ccxt\n",
    "\n",
    "from tensorforce.agents import Agent\n",
    "from tensorforce.environments import Environment\n",
    "\n",
    "from tensortrade.environments import TradingEnvironment\n",
    "from tensortrade.exchanges.simulated import FBMExchange\n",
    "from tensortrade.features.scalers import MinMaxNormalizer\n",
    "from tensortrade.features.stationarity import FractionalDifference\n",
    "from tensortrade.features import FeaturePipeline\n",
    "from tensortrade.rewards import SimpleProfitStrategy\n",
    "from tensortrade.actions import DiscreteActionStrategy\n",
    "from tensortrade.strategies import TensorforceTradingStrategy\n",
    "\n",
    "normalize = MinMaxNormalizer(inplace=True)\n",
    "difference = FractionalDifference(difference_order=0.6,\n",
    "                                  inplace=True)\n",
    "feature_pipeline = FeaturePipeline(steps=[normalize, difference])\n",
    "\n",
    "reward_strategy = SimpleProfitStrategy()\n",
    "action_strategy = DiscreteActionStrategy(n_actions=20, instrument_symbol='ETH/BTC')\n",
    "\n",
    "exchange = FBMExchange(base_instrument='BTC',\n",
    "                       timeframe='1h',\n",
    "                       should_pretransform_obs=True)\n",
    "\n",
    "environment = {\"exchange\": exchange,\n",
    "               \"action_strategy\": action_strategy,\n",
    "               \"reward_strategy\": reward_strategy,\n",
    "               \"feature_pipeline\": feature_pipeline}\n",
    "\n",
    "agent_spec = {\n",
    "    \"type\": \"ppo\",\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"discount\": 0.99,\n",
    "    \"likelihood_ratio_clipping\": 0.2,\n",
    "    \"max_episode_timesteps\": 10000\n",
    "}\n",
    "\n",
    "\n",
    "network_spec = [\n",
    "    dict(type='dense', size=64, activation=\"tanh\"),\n",
    "    dict(type='dense', size=32, activation=\"tanh\")\n",
    "]\n",
    "\n",
    "strategy = TensorforceTradingStrategy(environment=environment, agent_spec=agent_spec, network_spec=network_spec)\n",
    "\n",
    "performance = strategy.run(episodes=1, evaluation=False)\n",
    "\n",
    "performance[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensortrade]",
   "language": "python",
   "name": "conda-env-tensortrade-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
